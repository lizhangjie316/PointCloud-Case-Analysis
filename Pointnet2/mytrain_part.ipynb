{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97290ec6-2640-4ce7-9859-376c4323c57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "from data_utils.ShapeNetDataLoader import PartNormalDataset\n",
    "import torch\n",
    "import datetime\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import importlib\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import provider\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e72b129-ea67-4cdb-a495-5e346900c312",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "训练所需设置参数：\n",
    "--model pointnet2_part_seg_msg \n",
    "--normal \n",
    "--log_dir pointnet2_part_seg_msg\n",
    "\"\"\"\n",
    "BASE_DIR = os.path.dirname(os.path.realpath('__file__'))\n",
    "# BASE_DIR = os.path.dirname(os.path.abspath(__file__)) # 'C:\\\\Users\\\\Keen\\\\Desktop\\\\Project\\\\Github\\\\PointCloud-Case-Analysis\\\\Pointnet2'\n",
    "ROOT_DIR = BASE_DIR # 'C:\\\\Users\\\\Keen\\\\Desktop\\\\Project\\\\Github\\\\PointCloud-Case-Analysis\\\\Pointnet2'\n",
    "# print(ROOT_DIR)\n",
    "sys.path.append(os.path.join(ROOT_DIR, 'models'))  # 项目的其实根目录\n",
    "\n",
    "seg_classes = {'Earphone': [16, 17, 18], 'Motorbike': [30, 31, 32, 33, 34, 35], 'Rocket': [41, 42, 43], 'Car': [8, 9, 10, 11], 'Laptop': [28, 29], 'Cap': [6, 7], 'Skateboard': [44, 45, 46], 'Mug': [36, 37], 'Guitar': [19, 20, 21], 'Bag': [4, 5], 'Lamp': [24, 25, 26, 27], 'Table': [47, 48, 49], 'Airplane': [0, 1, 2, 3], 'Pistol': [38, 39, 40], 'Chair': [12, 13, 14, 15], 'Knife': [22, 23]}\n",
    "seg_label_to_cat = {}  # {0:Airplane, 1:Airplane, ...49:Table}   {0:Airplane, 1:Airplane, ...49:Table}\n",
    "for cat in seg_classes.keys():  # 将每个种类的 部件标签重置为方便理解的\n",
    "    for label in seg_classes[cat]:\n",
    "        seg_label_to_cat[label] = cat\n",
    "\n",
    "def to_categorical(y, num_classes):\n",
    "    \"\"\" 1-hot encodes a tensor \"\"\"\n",
    "    new_y = torch.eye(num_classes)[y.cpu().data.numpy(),]\n",
    "    if (y.is_cuda):\n",
    "        return new_y.cuda()  # torch.Size([4, 1, 16])\n",
    "    return new_y\n",
    "\n",
    "# --model pointnet2_part_seg_msg --normal --log_dir pointnet2_part_seg_msg\n",
    "def parse_args():  # 参数解析\n",
    "    parser = argparse.ArgumentParser('Model')\n",
    "    parser.add_argument('--model', type=str, default='pointnet2_part_seg_msg', help='model name [default: pointnet2_part_seg_msg]')\n",
    "    parser.add_argument('--batch_size', type=int, default=4, help='Batch Size during training [default: 16]') # 本机能承受12\n",
    "    parser.add_argument('--epoch',  default=251, type=int, help='Epoch to run [default: 251]')\n",
    "    parser.add_argument('--learning_rate', default=0.001, type=float, help='Initial learning rate [default: 0.001]')\n",
    "    parser.add_argument('--gpu', type=str, default='0', help='GPU to use [default: GPU 0]')\n",
    "    parser.add_argument('--optimizer', type=str, default='Adam', help='Adam or SGD [default: Adam]')\n",
    "    parser.add_argument('--log_dir', type=str, default='2021-06-23-20-58-part', help='Log path [default: None]')  # pointnet2_part_seg_msg\n",
    "    parser.add_argument('--decay_rate', type=float, default=1e-4, help='weight decay [default: 1e-4]')\n",
    "    parser.add_argument('--npoint', type=int,  default=2048, help='Point Number [default: 2048]')\n",
    "    parser.add_argument('--normal', action='store_true', default=True, help='Whether to use normal information [default: False]')\n",
    "    parser.add_argument('--step_size', type=int,  default=20, help='Decay step for lr decay [default: every 20 epochs]')\n",
    "    parser.add_argument('--lr_decay', type=float,  default=0.5, help='Decay rate for lr decay [default: 0.5]')\n",
    "\n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b8e49b2-93b1-418c-beee-66c6a3b9610e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "# args(batch_size=12, decay_rate=0.0001, epoch=251, gpu='0', learning_rate=0.001, log_dir='2021-06-05_19-52_pointnet2_part_seg_msg', lr_decay=0.5, model='pointnet2_part_seg_msg', normal=True, npoint=2048, optimizer='Adam', step_size=20)\n",
    "    def log_string(str):\n",
    "        logger.info(str)\n",
    "        # print(str)\n",
    "\n",
    "    '''HYPER PARAMETER'''\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu  # 0号GPU\n",
    "\n",
    "    '''CREATE DIR'''  # 创建log存放的文件目录及文件夹，存储在log目录下\n",
    "    timestr = str(datetime.datetime.now().strftime('%Y-%m-%d_%H-%M'))\n",
    "    experiment_dir = Path('./log/')\n",
    "    experiment_dir.mkdir(exist_ok=True)\n",
    "    experiment_dir = experiment_dir.joinpath('part_seg')\n",
    "    experiment_dir.mkdir(exist_ok=True)\n",
    "    if args.log_dir is None:\n",
    "        experiment_dir = experiment_dir.joinpath(timestr)\n",
    "    else:\n",
    "        experiment_dir = experiment_dir.joinpath(args.log_dir)\n",
    "    experiment_dir.mkdir(exist_ok=True)\n",
    "    checkpoints_dir = experiment_dir.joinpath('checkpoints/')\n",
    "    checkpoints_dir.mkdir(exist_ok=True)\n",
    "    log_dir = experiment_dir.joinpath('logs/')\n",
    "    log_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    '''LOG'''\n",
    "    args = parse_args()\n",
    "    logger = logging.getLogger(\"Model\")\n",
    "    logger.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    file_handler = logging.FileHandler('%s/%s.txt' % (log_dir, args.model))\n",
    "    file_handler.setLevel(logging.INFO)\n",
    "    file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(file_handler)\n",
    "    log_string('PARAMETER ...')\n",
    "    log_string(args)  # Namespace(batch_size=4, decay_rate=0.0001, epoch=251, gpu='0', learning_rate=0.001, log_dir='pointnet2_part_seg_msg', lr_decay=0.5, model='pointnet2_part_seg_msg', normal=True, npoint=2048, optimizer='Adam', step_size=20)\n",
    "\n",
    "    root = 'data/shapenetcore_partanno_segmentation_benchmark_v0_normal/'\n",
    "\n",
    "    # 开始处理数据集\n",
    "    # 返回2048个点，并进行正则化   提前已经分配好了哪些是训练集，哪些作为测试集\n",
    "    TRAIN_DATASET = PartNormalDataset(root = root, npoints=args.npoint, split='trainval', normal_channel=args.normal)\n",
    "\n",
    "    # 按照batch_size进行组装数据\n",
    "    trainDataLoader = torch.utils.data.DataLoader(TRAIN_DATASET, batch_size=args.batch_size, shuffle=True, num_workers=4)\n",
    "    # 测试数据同样处理\n",
    "    TEST_DATASET = PartNormalDataset(root = root, npoints=args.npoint, split='test', normal_channel=args.normal)\n",
    "    testDataLoader = torch.utils.data.DataLoader(TEST_DATASET, batch_size=args.batch_size,shuffle=False, num_workers=4)\n",
    "\n",
    "    log_string(\"The number of training data is: %d\" % len(TRAIN_DATASET))  # 训练数据 13998\n",
    "    log_string(\"The number of test data is: %d\" %  len(TEST_DATASET))  # 测试数据 2874\n",
    "    num_classes = 16\n",
    "    num_part = 50\n",
    "    '''MODEL LOADING'''\n",
    "    MODEL = importlib.import_module(args.model)  # 'pointnet2_part_seg_msg'\n",
    "    # 将模型和工具包都添加到log文件中\n",
    "    shutil.copy('models/%s.py' % args.model, str(experiment_dir))\n",
    "    shutil.copy('models/pointnet_util.py', str(experiment_dir))\n",
    "    # 分类器，进行50分类，对2048个点都要进行分类\n",
    "    classifier = MODEL.get_model(num_part, normal_channel=args.normal).cuda()\n",
    "    criterion = MODEL.get_loss().cuda()  # 计算损失函数的方式\n",
    "\n",
    "\n",
    "    def weights_init(m):\n",
    "        classname = m.__class__.__name__\n",
    "        if classname.find('Conv2d') != -1:\n",
    "            torch.nn.init.xavier_normal_(m.weight.data)\n",
    "            torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "        elif classname.find('Linear') != -1:\n",
    "            torch.nn.init.xavier_normal_(m.weight.data)\n",
    "            torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "    try:  # 加载预训练的模型\n",
    "        checkpoint = torch.load(str(experiment_dir) + '/checkpoints/best_model.pth')\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        classifier.load_state_dict(checkpoint['model_state_dict'])\n",
    "        log_string('Use pretrain model')\n",
    "    except:\n",
    "        log_string('No existing model, starting training from scratch...')\n",
    "        start_epoch = 0\n",
    "        classifier = classifier.apply(weights_init)\n",
    "\n",
    "    if args.optimizer == 'Adam': #TODO 研究这些参数\n",
    "        optimizer = torch.optim.Adam(\n",
    "            classifier.parameters(),\n",
    "            lr=args.learning_rate,\n",
    "            betas=(0.9, 0.999),\n",
    "            eps=1e-08,\n",
    "            weight_decay=args.decay_rate\n",
    "        )\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(classifier.parameters(), lr=args.learning_rate, momentum=0.9)\n",
    "    # 依据动量进行调整\n",
    "    def bn_momentum_adjust(m, momentum):\n",
    "        if isinstance(m, torch.nn.BatchNorm2d) or isinstance(m, torch.nn.BatchNorm1d):\n",
    "            m.momentum = momentum\n",
    "\n",
    "    LEARNING_RATE_CLIP = 1e-5  # 1e-05\n",
    "    MOMENTUM_ORIGINAL = 0.1  # 0.1\n",
    "    MOMENTUM_DECCAY = 0.5  # 0.5\n",
    "    MOMENTUM_DECCAY_STEP = args.step_size  # 20\n",
    "\n",
    "    best_acc = 0\n",
    "    global_epoch = 0\n",
    "    best_class_avg_iou = 0\n",
    "    best_instance_avg_iou = 0\n",
    "\n",
    "    # 开始进行迭代训练\n",
    "    for epoch in range(start_epoch,args.epoch):\n",
    "        log_string('Epoch %d (%d/%s):' % (global_epoch + 1, epoch + 1, args.epoch))\n",
    "        '''Adjust learning rate and BN momentum''' # 学习率最小为LEARNING_RATE_CLIP即1e-5；每20个epoch，lr_decay即0.5^(epoch//20),并取最大\n",
    "        lr = max(args.learning_rate * (args.lr_decay ** (epoch // args.step_size)), LEARNING_RATE_CLIP)\n",
    "        log_string('Learning rate:%f' % lr)\n",
    "        #TODO：？？？\n",
    "        # param_groups 是一个list，里面每一个item都是字典；这项作用是给内部的lr项赋值为param上的lr\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        mean_correct = []\n",
    "        # 0.1*（0.5^（epoch//20））  每20步，动量减小一次\n",
    "        momentum = MOMENTUM_ORIGINAL * (MOMENTUM_DECCAY ** (epoch // MOMENTUM_DECCAY_STEP))\n",
    "        if momentum < 0.01:\n",
    "            momentum = 0.01\n",
    "        print('BN momentum updated to: %f' % momentum)  # 0.100000\n",
    "        classifier = classifier.apply(lambda x: bn_momentum_adjust(x,momentum))\n",
    "\n",
    "        '''learning one epoch'''\n",
    "        for i, data in tqdm(enumerate(trainDataLoader), total=len(trainDataLoader), smoothing=0.9):\n",
    "            points, label, target = data  # 4个点、 label：代表点的种类类别  target：代表部件类别\n",
    "            # print(points.shape) # (4,2048,6)   label.shape: (4,1)  target.shape: (4,2048)\n",
    "            # 数据增强：做一些微小扰动\n",
    "            points = points.data.numpy()\n",
    "            # print(points.shape)  # (4,2048,6)\n",
    "            points[:,:, 0:3] = provider.random_scale_point_cloud(points[:,:, 0:3])  # 增加一些扰动，从而进行数据增强\n",
    "            points[:,:, 0:3] = provider.shift_point_cloud(points[:, :, 0:3])\n",
    "            points = torch.Tensor(points)\n",
    "            points, label, target = points.float().cuda(),label.long().cuda(), target.long().cuda()\n",
    "            # print(points.shape)  # torch.Size([4, 2048, 6])\n",
    "            # print(label.shape)  # torch.Size([4, 1])   每个样本的对应的种类标签\n",
    "            # print(target.shape)  # torch.Size([4, 2048])   每个点的类别标签\n",
    "            points = points.transpose(2, 1)  # 通道前置\n",
    "            # print(points.shape)  # torch.Size([4, 6, 2048])\n",
    "            optimizer.zero_grad()\n",
    "            classifier = classifier.train()  # torch.Size([4, 6, 2048])       [4,1,16]\n",
    "            seg_pred, trans_feat = classifier(points, to_categorical(label, num_classes))  # seg_pred  torch.Size([4, 2048, 50])   trans_feat：torch.Size([4, 1024, 1])\n",
    "            seg_pred = seg_pred.contiguous().view(-1, num_part)  # torch.Size([8192, 50])\n",
    "            target = target.view(-1, 1)[:, 0]  # [8192]\n",
    "            pred_choice = seg_pred.data.max(1)[1]  # 8192  预测的结果部件类别\n",
    "            correct = pred_choice.eq(target.data).cpu().sum()  # tensor(249)   即只有249个正确\n",
    "            mean_correct.append(correct.item() / (args.batch_size * args.npoint))  # 平均正确率  0.0303955078125\n",
    "            loss = criterion(seg_pred, target, trans_feat)  # seg_pred：[8192, 50]  target：[8192]  trans_feat：[4, 1024, 1]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_instance_acc = np.mean(mean_correct)  # 1个epoch 准确率  mean_correct的list中有 3500个值  13998个样本，一个batch处理4个,共需3500步 step\n",
    "        log_string('Train accuracy is: %.5f' % train_instance_acc)  # 实例分割 准确率： 0.8502310616629464\n",
    "        # 进行测试\n",
    "        with torch.no_grad():  # 非训练过程，后续的tensor操作，不需要进行计算图的构建（计算过程的构建，以便梯度反向传播等操作），只用来进行测试\n",
    "            test_metrics = {}\n",
    "            total_correct = 0\n",
    "            total_seen = 0\n",
    "            total_seen_class = [0 for _ in range(num_part)]  # num_part个0组成的list\n",
    "            total_correct_class = [0 for _ in range(num_part)]\n",
    "            shape_ious = {cat: [] for cat in seg_classes.keys()}\n",
    "            seg_label_to_cat = {}  # {0:Airplane, 1:Airplane, ...49:Table}\n",
    "            for cat in seg_classes.keys():\n",
    "                for label in seg_classes[cat]:  # 每种的部件类别\n",
    "                    seg_label_to_cat[label] = cat\n",
    "\n",
    "            for batch_id, (points, label, target) in tqdm(enumerate(testDataLoader), total=len(testDataLoader), smoothing=0.9):\n",
    "                cur_batch_size, NUM_POINT, _ = points.size()  # torch.Size([4, 2048, 6])\n",
    "                points, label, target = points.float().cuda(), label.long().cuda(), target.long().cuda()\n",
    "                points = points.transpose(2, 1)  # torch.Size([4, 6, 2048])\n",
    "                classifier = classifier.eval()\n",
    "                seg_pred, _ = classifier(points, to_categorical(label, num_classes)) # torch.Size([4, 2048, 50])\n",
    "                cur_pred_val = seg_pred.cpu().data.numpy()  # (4, 2048, 50)\n",
    "                cur_pred_val_logits = cur_pred_val\n",
    "                cur_pred_val = np.zeros((cur_batch_size, NUM_POINT)).astype(np.int32)  # (4, 2048)\n",
    "                target = target.cpu().data.numpy()  # 部件的类别一个batch中 所有点的类别  (4, 2048)\n",
    "                for i in range(cur_batch_size):  # 对每个实例样本\n",
    "                    cat = seg_label_to_cat[target[i, 0]]  # 获取每一个点其所对应的实例类别\n",
    "                    logits = cur_pred_val_logits[i, :, :]    # (2048, 50)\n",
    "                    cur_pred_val[i, :] = np.argmax(logits[:, seg_classes[cat]], 1) + seg_classes[cat][0]  # argmax 取出logits[:, seg_classes[cat]], 1)中元素最大值的索引，\n",
    "                correct = np.sum(cur_pred_val == target)  # 7200\n",
    "                total_correct += correct  # 当前正确点的总和\n",
    "                total_seen += (cur_batch_size * NUM_POINT)  # 当前总的可见点，已经推理过的点\n",
    "                # 每个部件进行统计\n",
    "                for l in range(num_part):\n",
    "                    total_seen_class[l] += np.sum(target == l)  # 每个部件类别总的 需要判断的点\n",
    "                    total_correct_class[l] += (np.sum((cur_pred_val == l) & (target == l)))  # 每个部件类别正确的点\n",
    "\n",
    "                for i in range(cur_batch_size):\n",
    "                    segp = cur_pred_val[i, :]  # (4, 2048)\n",
    "                    segl = target[i, :]\n",
    "                    cat = seg_label_to_cat[segl[0]]  # 任意一个部件类别，即可确定一个实例类别\n",
    "                    part_ious = [0.0 for _ in range(len(seg_classes[cat]))]  # 实例类别cat有多少个子类别，生成同尺寸的0.0的列表\n",
    "                    for l in seg_classes[cat]:\n",
    "                        if (np.sum(segl == l) == 0) and (np.sum(segp == l) == 0):  # part is not present, no prediction as well\n",
    "                            part_ious[l - seg_classes[cat][0]] = 1.0\n",
    "                        else:\n",
    "                            part_ious[l - seg_classes[cat][0]] = np.sum((segl == l) & (segp == l)) / float(np.sum((segl == l) | (segp == l)))\n",
    "                    shape_ious[cat].append(np.mean(part_ious))  # 每个样本的平均部件iou\n",
    "\n",
    "            all_shape_ious = []\n",
    "            for cat in shape_ious.keys():  # 计算所有shape的部件 实例iou\n",
    "                for iou in shape_ious[cat]:\n",
    "                    all_shape_ious.append(iou)\n",
    "                shape_ious[cat] = np.mean(shape_ious[cat])  # 每个shape的平均实例iou  cmiou\n",
    "            mean_shape_ious = np.mean(list(shape_ious.values()))\n",
    "            test_metrics['accuracy'] = total_correct / float(total_seen)\n",
    "            test_metrics['class_avg_accuracy'] = np.mean( # 没有使用到\n",
    "                np.array(total_correct_class) / np.array(total_seen_class, dtype=np.float))\n",
    "            for cat in sorted(shape_ious.keys()):  # 每种类别的准确率\n",
    "                log_string('eval mIoU of %s %f' % (cat + ' ' * (14 - len(cat)), shape_ious[cat]))\n",
    "            test_metrics['class_avg_iou'] = mean_shape_ious\n",
    "            test_metrics['instance_avg_iou'] = np.mean(all_shape_ious)\n",
    "\n",
    "        # 如果当前epoch测试效果不好，输出的仍旧是上轮的结果\n",
    "        log_string('Epoch %d test Accuracy: %f  Class avg mIOU: %f   Instance avg mIOU: %f' % (\n",
    "                 epoch+1, test_metrics['accuracy'],test_metrics['class_avg_iou'],test_metrics['instance_avg_iou']))\n",
    "        if (test_metrics['instance_avg_iou'] >= best_instance_avg_iou):\n",
    "            logger.info('Save model...')\n",
    "            savepath = str(checkpoints_dir) + '/best_model.pth'\n",
    "            log_string('Saving at %s'% savepath)\n",
    "            state = {\n",
    "                'epoch': epoch,\n",
    "                'train_acc': train_instance_acc,\n",
    "                'test_acc': test_metrics['accuracy'],\n",
    "                'class_avg_iou': test_metrics['class_avg_iou'],\n",
    "                'instance_avg_iou': test_metrics['instance_avg_iou'],\n",
    "                'model_state_dict': classifier.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }\n",
    "            torch.save(state, savepath)\n",
    "            log_string('Saving model....')\n",
    "\n",
    "        if test_metrics['accuracy'] > best_acc:\n",
    "            best_acc = test_metrics['accuracy']\n",
    "        if test_metrics['class_avg_iou'] > best_class_avg_iou:\n",
    "            best_class_avg_iou = test_metrics['class_avg_iou']\n",
    "        if test_metrics['instance_avg_iou'] > best_instance_avg_iou:\n",
    "            best_instance_avg_iou = test_metrics['instance_avg_iou']\n",
    "        log_string('Best accuracy is: %.5f'%best_acc)\n",
    "        log_string('Best class avg mIOU is: %.5f'%best_class_avg_iou)\n",
    "        log_string('Best instance avg mIOU is: %.5f' % best_instance_avg_iou)\n",
    "        global_epoch+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e7240bc-9389-4001-9a50-6c969a0f1080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: Model [-h] [--model MODEL] [--batch_size BATCH_SIZE] [--epoch EPOCH]\n",
      "             [--learning_rate LEARNING_RATE] [--gpu GPU]\n",
      "             [--optimizer OPTIMIZER] [--log_dir LOG_DIR]\n",
      "             [--decay_rate DECAY_RATE] [--npoint NPOINT] [--normal]\n",
      "             [--step_size STEP_SIZE] [--lr_decay LR_DECAY]\n",
      "Model: error: unrecognized arguments: -f C:\\Users\\Keen\\AppData\\Roaming\\jupyter\\runtime\\kernel-d2b088f7-b384-4a51-9034-e5893cd957b8.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "# python train_partseg.py --model pointnet2_part_seg_msg --normal --log_dir pointnet2_part_seg_msg\n",
    "if __name__ == '__main__':\n",
    "    print(\"hello\")\n",
    "    args = parse_args()\n",
    "#     main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a8c3411-88db-4055-88b0-069d4d27fa05",
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-02b218a164b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"hello\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparse_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m#     main(args)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-5a4dbd861db3>\u001b[0m in \u001b[0;36mparse_args\u001b[1;34m()\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'--lr_decay'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mdefault\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhelp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Decay rate for lr decay [default: 0.5]'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\envs\\pytorch1.6_cuda10.1\\lib\\argparse.py\u001b[0m in \u001b[0;36mparse_args\u001b[1;34m(self, args, namespace)\u001b[0m\n\u001b[0;32m   1756\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0margv\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1757\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'unrecognized arguments: %s'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1758\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1759\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1760\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\envs\\pytorch1.6_cuda10.1\\lib\\argparse.py\u001b[0m in \u001b[0;36merror\u001b[1;34m(self, message)\u001b[0m\n\u001b[0;32m   2506\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_usage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_sys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2507\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'prog'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'message'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2508\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%(prog)s: error: %(message)s\\n'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\envs\\pytorch1.6_cuda10.1\\lib\\argparse.py\u001b[0m in \u001b[0;36mexit\u001b[1;34m(self, status, message)\u001b[0m\n\u001b[0;32m   2493\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2494\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_print_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_sys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2495\u001b[1;33m         \u001b[0m_sys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2496\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2497\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSystemExit\u001b[0m: 2"
     ]
    }
   ],
   "source": [
    "%tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab685f32-9a42-47aa-a2a9-9b749d068220",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch1.6_cuda10.1",
   "language": "python",
   "name": "pytorch1.6_cuda10.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
